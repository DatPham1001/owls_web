{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "465eb2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train.txt...\n",
      "{'tokens': [['Tôi', 'muốn', 'mua', 'điện', 'thoại', 'dưới', '200', 'đô'], ['Tìm', 'máy', 'lạnh', 'giá', 'trên', '10', 'triệu'], ['Có', 'tủ', 'lạnh', 'nào', 'dưới', '5', 'triệu', 'không'], ['Tôi', 'cần', 'một', 'laptop', 'trên', '15', 'triệu'], ['Cho', 'hỏi', 'máy', 'giặt', 'dưới', '300', 'đô', 'có', 'không'], ['Tìm', 'máy', 'tính', 'bảng', 'giá', 'dưới', '7', 'triệu'], ['Mình', 'đang', 'tìm', 'tivi', 'trên', '12', 'triệu'], ['Có', 'máy', 'ảnh', 'nào', 'giá', 'dưới', '10', 'triệu', 'không'], ['Mua', 'loa', 'bluetooth', 'dưới', '1', 'triệu'], ['Mình', 'muốn', 'mua', 'điện', 'thoại', 'giá', 'cao', 'hơn', '10', 'triệu'], ['Tôi', 'muốn', 'mua', 'điện', 'thoại', 'dưới', '200', 'đô'], ['Tìm', 'máy', 'lạnh', 'giá', 'trên', '10', 'triệu'], ['Có', 'tủ', 'lạnh', 'nào', 'dưới', '5', 'triệu', 'không'], ['Tôi', 'cần', 'một', 'laptop', 'trên', '15', 'triệu'], ['Cho', 'hỏi', 'máy', 'giặt', 'dưới', '300', 'đô', 'có', 'không'], ['Tìm', 'máy', 'tính', 'bảng', 'giá', 'dưới', '7', 'triệu'], ['Mình', 'đang', 'tìm', 'tivi', 'trên', '12', 'triệu'], ['Có', 'máy', 'ảnh', 'nào', 'giá', 'dưới', '10', 'triệu', 'không'], ['Mua', 'loa', 'bluetooth', 'dưới', '1', 'triệu'], ['Mình', 'muốn', 'mua', 'điện', 'thoại', 'giá', 'cao', 'hơn', '10', 'triệu'], ['Tôi', 'muốn', 'mua', 'điện', 'thoại', 'dưới', '200', 'đô'], ['Tìm', 'máy', 'lạnh', 'giá', 'trên', '10', 'triệu'], ['Có', 'tủ', 'lạnh', 'nào', 'dưới', '5', 'triệu', 'không'], ['Tôi', 'cần', 'một', 'laptop', 'trên', '15', 'triệu'], ['Cho', 'hỏi', 'máy', 'giặt', 'dưới', '300', 'đô', 'có', 'không'], ['Tìm', 'máy', 'tính', 'bảng', 'giá', 'dưới', '7', 'triệu'], ['Mình', 'đang', 'tìm', 'tivi', 'trên', '12', 'triệu'], ['Có', 'máy', 'ảnh', 'nào', 'giá', 'dưới', '10', 'triệu', 'không'], ['Mua', 'loa', 'bluetooth', 'dưới', '1', 'triệu'], ['Mình', 'muốn', 'mua', 'điện', 'thoại', 'giá', 'cao', 'hơn', '10', 'triệu'], ['Tôi', 'muốn', 'mua', 'điện', 'thoại', 'dưới', '200', 'đô'], ['Tìm', 'máy', 'lạnh', 'giá', 'trên', '10', 'triệu'], ['Có', 'tủ', 'lạnh', 'nào', 'dưới', '5', 'triệu', 'không'], ['Tôi', 'cần', 'một', 'laptop', 'trên', '15', 'triệu'], ['Cho', 'hỏi', 'máy', 'giặt', 'dưới', '300', 'đô', 'có', 'không'], ['Tìm', 'máy', 'tính', 'bảng', 'giá', 'dưới', '7', 'triệu'], ['Mình', 'đang', 'tìm', 'tivi', 'trên', '12', 'triệu'], ['Có', 'máy', 'ảnh', 'nào', 'giá', 'dưới', '10', 'triệu', 'không'], ['Mua', 'loa', 'bluetooth', 'dưới', '1', 'triệu'], ['Mình', 'muốn', 'mua', 'điện', 'thoại', 'giá', 'cao', 'hơn', '10', 'triệu'], ['Tôi', 'muốn', 'mua', 'điện', 'thoại', 'dưới', '200', 'đô'], ['Tìm', 'máy', 'lạnh', 'giá', 'trên', '10', 'triệu'], ['Có', 'tủ', 'lạnh', 'nào', 'dưới', '5', 'triệu', 'không'], ['Tôi', 'cần', 'một', 'laptop', 'trên', '15', 'triệu'], ['Cho', 'hỏi', 'máy', 'giặt', 'dưới', '300', 'đô', 'có', 'không'], ['Tìm', 'máy', 'tính', 'bảng', 'giá', 'dưới', '7', 'triệu'], ['Mình', 'đang', 'tìm', 'tivi', 'trên', '12', 'triệu'], ['Có', 'máy', 'ảnh', 'nào', 'giá', 'dưới', '10', 'triệu', 'không'], ['Mua', 'loa', 'bluetooth', 'dưới', '1', 'triệu'], ['Mình', 'muốn', 'mua', 'điện', 'thoại', 'giá', 'cao', 'hơn', '10', 'triệu'], ['Tôi', 'muốn', 'mua', 'điện', 'thoại', 'dưới', '200', 'đô'], ['Tìm', 'máy', 'lạnh', 'giá', 'trên', '10', 'triệu'], ['Có', 'tủ', 'lạnh', 'nào', 'dưới', '5', 'triệu', 'không'], ['Tôi', 'cần', 'một', 'laptop', 'trên', '15', 'triệu'], ['Cho', 'hỏi', 'máy', 'giặt', 'dưới', '300', 'đô', 'có', 'không'], ['Tìm', 'máy', 'tính', 'bảng', 'giá', 'dưới', '7', 'triệu'], ['Mình', 'đang', 'tìm', 'tivi', 'trên', '12', 'triệu'], ['Có', 'máy', 'ảnh', 'nào', 'giá', 'dưới', '10', 'triệu', 'không'], ['Mua', 'loa', 'bluetooth', 'dưới', '1', 'triệu'], ['Mình', 'muốn', 'mua', 'điện', 'thoại', 'giá', 'cao', 'hơn', '10', 'triệu'], ['Tôi', 'muốn', 'mua', 'điện', 'thoại', 'dưới', '200', 'đô'], ['Tìm', 'máy', 'lạnh', 'giá', 'trên', '10', 'triệu'], ['Có', 'tủ', 'lạnh', 'nào', 'dưới', '5', 'triệu', 'không'], ['Tôi', 'cần', 'một', 'laptop', 'trên', '15', 'triệu'], ['Cho', 'hỏi', 'máy', 'giặt', 'dưới', '300', 'đô', 'có', 'không'], ['Tìm', 'máy', 'tính', 'bảng', 'giá', 'dưới', '7', 'triệu'], ['Mình', 'đang', 'tìm', 'tivi', 'trên', '12', 'triệu'], ['Có', 'máy', 'ảnh', 'nào', 'giá', 'dưới', '10', 'triệu', 'không'], ['Mua', 'loa', 'bluetooth', 'dưới', '1', 'triệu'], ['Mình', 'muốn', 'mua', 'điện', 'thoại', 'giá', 'cao', 'hơn', '10', 'triệu'], ['Tôi', 'muốn', 'mua', 'điện', 'thoại', 'dưới', '200', 'đô'], ['Tìm', 'máy', 'lạnh', 'giá', 'trên', '10', 'triệu'], ['Có', 'tủ', 'lạnh', 'nào', 'dưới', '5', 'triệu', 'không'], ['Tôi', 'cần', 'một', 'laptop', 'trên', '15', 'triệu'], ['Cho', 'hỏi', 'máy', 'giặt', 'dưới', '300', 'đô', 'có', 'không'], ['Tìm', 'máy', 'tính', 'bảng', 'giá', 'dưới', '7', 'triệu'], ['Mình', 'đang', 'tìm', 'tivi', 'trên', '12', 'triệu'], ['Có', 'máy', 'ảnh', 'nào', 'giá', 'dưới', '10', 'triệu', 'không'], ['Mua', 'loa', 'bluetooth', 'dưới', '1', 'triệu'], ['Mình', 'muốn', 'mua', 'điện', 'thoại', 'giá', 'cao', 'hơn', '10', 'triệu']], 'ner_tags': [['O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O'], ['O', 'O', 'O', 'B-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O', 'O'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'O', 'B-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'I-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O'], ['O', 'O', 'O', 'B-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O', 'O'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'O', 'B-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'I-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O'], ['O', 'O', 'O', 'B-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O', 'O'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'O', 'B-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'I-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O'], ['O', 'O', 'O', 'B-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O', 'O'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'O', 'B-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'I-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O'], ['O', 'O', 'O', 'B-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O', 'O'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'O', 'B-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'I-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O'], ['O', 'O', 'O', 'B-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O', 'O'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'O', 'B-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'I-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O'], ['O', 'O', 'O', 'B-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O', 'O'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'O', 'B-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'I-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O'], ['O', 'O', 'O', 'B-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O', 'O'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'O', 'B-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE', 'O'], ['O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'B-COMPARE', 'I-COMPARE', 'B-PRICE', 'I-PRICE']]}\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/80 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map:   0%|          | 0/80 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "word_ids() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast` class).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 71\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_inputs\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizing dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m tokenized_datasets \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_and_align_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Map string labels to IDs for model\u001b[39;00m\n\u001b[0;32m     74\u001b[0m label_to_id \u001b[38;5;241m=\u001b[39m {label: i \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(label_list)}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\dataset_dict.py:941\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[0;32m    939\u001b[0m     function \u001b[38;5;241m=\u001b[39m bind(function, split)\n\u001b[1;32m--> 941\u001b[0m dataset_dict[split] \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[0;32m    962\u001b[0m     function \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunc\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    555\u001b[0m }\n\u001b[0;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py:3074\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3070\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3071\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3072\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3073\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3074\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3075\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3076\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py:3516\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3514\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3515\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 3516\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_examples_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3518\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py:3466\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.iter_outputs\u001b[1;34m(shard_iterable)\u001b[0m\n\u001b[0;32m   3464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3465\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[1;32m-> 3466\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\datasets\\arrow_dataset.py:3389\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function\u001b[1;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[0;32m   3387\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[0;32m   3388\u001b[0m inputs, fn_args, additional_args, fn_kwargs \u001b[38;5;241m=\u001b[39m prepare_inputs(pa_inputs, indices, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[1;32m-> 3389\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "Cell \u001b[1;32mIn[10], line 59\u001b[0m, in \u001b[0;36mtokenize_and_align_labels\u001b[1;34m(examples)\u001b[0m\n\u001b[0;32m     57\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m---> 59\u001b[0m     word_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenized_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     label_ids \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word_idx \u001b[38;5;129;01min\u001b[39;00m word_ids:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:398\u001b[0m, in \u001b[0;36mBatchEncoding.word_ids\u001b[1;34m(self, batch_index)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;124;03mReturn a list mapping the tokens to their actual word in the initial sentence for a fast tokenizer.\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;124;03m    (several tokens will be mapped to the same word index if they are parts of that word).\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings:\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_ids() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    400\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    401\u001b[0m     )\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings[batch_index]\u001b[38;5;241m.\u001b[39mword_ids\n",
      "\u001b[1;31mValueError\u001b[0m: word_ids() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast` class)."
     ]
    }
   ],
   "source": [
    "# PhoBERT NER Simple Training Script with Debug Prints (Label-as-String)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Define labels as strings (no mapping to IDs)\n",
    "label_list = [\"O\", \"B-PRODUCT\", \"I-PRODUCT\", \"B-PRICE\", \"I-PRICE\", \"B-COMPARE\", \"I-COMPARE\"]\n",
    "\n",
    "# Load PhoBERT tokenizer and model using AutoTokenizer with fast=True\n",
    "model_checkpoint = \"vinai/phobert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "# Read and parse train.txt\n",
    "print(\"Reading train.txt...\")\n",
    "with open(\"./data/train.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().strip().split(\"\\n\")\n",
    "\n",
    "tokens = []\n",
    "labels = []\n",
    "sentence_tokens = []\n",
    "sentence_labels = []\n",
    "\n",
    "for line in lines:\n",
    "    if line == \"\":\n",
    "        if sentence_tokens:\n",
    "            tokens.append(sentence_tokens)\n",
    "            labels.append(sentence_labels)\n",
    "            sentence_tokens = []\n",
    "            sentence_labels = []\n",
    "    else:\n",
    "        splits = line.strip().split()\n",
    "        if len(splits) == 2:\n",
    "            token, label = splits\n",
    "            sentence_tokens.append(token)\n",
    "            sentence_labels.append(label)\n",
    "\n",
    "# Add last sentence if needed\n",
    "if sentence_tokens:\n",
    "    tokens.append(sentence_tokens)\n",
    "    labels.append(sentence_labels)\n",
    "\n",
    "train_examples = {\"tokens\": tokens, \"ner_tags\": labels}\n",
    "print(train_examples)\n",
    "\n",
    "# Create DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_dict(train_examples),\n",
    "    \"test\": Dataset.from_dict(train_examples),\n",
    "})\n",
    "\n",
    "# Tokenize and align labels\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(\"O\")\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Map string labels to IDs for model\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "\n",
    "def convert_labels_to_ids(examples):\n",
    "    examples[\"labels\"] = [[label_to_id[label] for label in seq] for seq in examples[\"labels\"]]\n",
    "    return examples\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(convert_labels_to_ids, batched=True)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list)).to(device)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Evaluation metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_predictions = [\n",
    "        [id_to_label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id_to_label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    print(\"Evaluation results:\", results)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phobert-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f6a73b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': [['Tôi', 'muốn', 'mua', 'điện thoại', 'dưới', '200', 'đô'], ['Tìm', 'máy lạnh', 'giá', 'trên', '10', 'triệu'], ['Có', 'tủ lạnh', 'nào', 'dưới', '5', 'triệu']], 'ner_tags': [['O', 'O', 'O', 'B-PRODUCT', 'B-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'O', 'O-COMPARE', 'B-PRICE', 'I-PRICE'], ['O', 'B-PRODUCT', 'O', 'B-COMPARE', 'B-PRICE', 'I-PRICE']]}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Đọc tệp CSV chứa dữ liệu\n",
    "data = pd.read_csv('./data/train_data.csv')\n",
    "\n",
    "# Chia dữ liệu thành các câu và nhãn\n",
    "sentences = []\n",
    "labels = []\n",
    "sentence = []\n",
    "label = []\n",
    "\n",
    "# Duyệt qua từng dòng trong CSV để tạo ra danh sách câu và nhãn\n",
    "for idx, row in data.iterrows():\n",
    "    token = row['Token']\n",
    "    tag = row['Label']\n",
    "    \n",
    "    # Thêm token vào câu và nhãn\n",
    "    sentence.append(token)\n",
    "    label.append(tag)\n",
    "    \n",
    "    # Khi gặp dòng cuối cùng của mỗi câu (có thể tùy chỉnh ở đây)\n",
    "    if token == 'triệu' or token == 'đô' or token == 'nghìn'  :  # Chỉ là ví dụ, điều chỉnh phù hợp với cấu trúc của bạn\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "        sentence = []\n",
    "        label = []\n",
    "\n",
    "# Chuyển đổi thành định dạng Dataset\n",
    "dataset = Dataset.from_dict({\n",
    "    'tokens': sentences,\n",
    "    'ner_tags': labels\n",
    "})\n",
    "\n",
    "# In ra mẫu\n",
    "print(dataset[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "077c8af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>                  O\n",
      "Tôi                  B-PRODUCT\n",
      "muốn                 B-PRODUCT\n",
      "mua                  B-PRODUCT\n",
      "điện                 O\n",
      "thoại                O\n",
      "dưới                 B-PRODUCT\n",
      "200                  B-PRODUCT\n",
      "đô                   O\n",
      "</s>                 B-PRODUCT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "phobert = AutoModelForTokenClassification.from_pretrained(\"vinai/phobert-base-v2\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "\n",
    "# Your label mapping (directly using label values, not ID)\n",
    "label_list = [\"O\", \"B-PRODUCT\", \"I-PRODUCT\", \"B-PRICE\", \"I-PRICE\", \"B-COMPARE\", \"I-COMPARE\"]\n",
    "\n",
    "# Word-segmented input sentence\n",
    "sentence = \"Tôi muốn mua điện thoại dưới 200 đô\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = tokenizer(sentence, return_tensors=\"pt\", truncation=True).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = phobert(**tokens)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "# Convert token IDs to corresponding labels\n",
    "token_ids = tokens[\"input_ids\"][0]\n",
    "predicted_labels = [label_list[pred.item()] for pred in predictions[0]]\n",
    "\n",
    "# Decode tokens\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "# Print the results\n",
    "for token, label in zip(decoded_tokens, predicted_labels):\n",
    "    print(f\"{token:<20} {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "257b5978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3/3 [00:00<00:00, 498.73 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 501.25 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`tokens` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:777\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[1;32m--> 777\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:739\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[1;34m(value, dtype)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[1;32m--> 739\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 103\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 103\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 701\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset, (IterDataPipe, MapDataPipe)):\n\u001b[0;32m    749\u001b[0m     \u001b[38;5;66;03m# For BC, use default SHARDING_PRIORITIES\u001b[39;00m\n\u001b[0;32m    750\u001b[0m     torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mgraph_settings\u001b[38;5;241m.\u001b[39mapply_sharding(\n\u001b[0;32m    751\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_world_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rank\n\u001b[0;32m    752\u001b[0m     )\n\u001b[0;32m    754\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher \u001b[38;5;241m=\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mcreate_fetcher(\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind,\n\u001b[0;32m    756\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset,\n\u001b[1;32m--> 757\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_auto_collation,\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collate_fn,\n\u001b[0;32m    759\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drop_last,\n\u001b[0;32m    760\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\data\\data_collator.py:46\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[1;34m(self, features, return_tensors)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_call(features)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy_call(features)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\data\\data_collator.py:334\u001b[0m, in \u001b[0;36mDataCollatorForTokenClassification.torch_call\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    330\u001b[0m labels \u001b[38;5;241m=\u001b[39m [feature[label_name] \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features] \u001b[38;5;28;01mif\u001b[39;00m label_name \u001b[38;5;129;01min\u001b[39;00m features[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    332\u001b[0m no_labels_features \u001b[38;5;241m=\u001b[39m [{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m label_name} \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m--> 334\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_labels_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\data\\data_collator.py:67\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[1;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:3407\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   3404\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   3405\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[1;32m-> 3407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:241\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    237\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:793\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    789\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    790\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    791\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    792\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m--> 793\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    794\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    796\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    797\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    798\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`tokens` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from torch.optim import AdamW\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Cấu hình ---\n",
    "label_list = [\"O\", \"B-PRODUCT\", \"I-PRODUCT\", \"B-PRICE\", \"I-PRICE\", \"B-COMPARE\", \"I-COMPARE\"]\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "model_checkpoint = \"vinai/phobert-base\"\n",
    "\n",
    "# --- Load tokenizer và model ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
    "\n",
    "# --- Tạo dữ liệu ---\n",
    "train_examples = {\n",
    "    \"tokens\": [\n",
    "        [\"Tôi\", \"muốn\", \"mua\", \"điện\", \"thoại\", \"dưới\", \"200\", \"đô\"],\n",
    "        [\"Tìm\", \"máy\", \"lạnh\", \"giá\", \"trên\", \"10\", \"triệu\"],\n",
    "        [\"Có\", \"tủ\", \"lạnh\", \"nào\", \"dưới\", \"5\", \"triệu\", \"không\"]\n",
    "    ],\n",
    "    \"ner_tags\": [\n",
    "        [\"O\", \"O\", \"O\", \"B-PRODUCT\", \"I-PRODUCT\", \"B-COMPARE\", \"B-PRICE\", \"I-PRICE\"],\n",
    "        [\"O\", \"B-PRODUCT\", \"I-PRODUCT\", \"O\", \"B-COMPARE\", \"B-PRICE\", \"I-PRICE\"],\n",
    "        [\"O\", \"B-PRODUCT\", \"I-PRODUCT\", \"O\", \"B-COMPARE\", \"B-PRICE\", \"I-PRICE\", \"O\"]\n",
    "    ]\n",
    "}\n",
    "\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": Dataset.from_dict(train_examples),\n",
    "    \"test\": Dataset.from_dict(train_examples),\n",
    "})\n",
    "\n",
    "# --- Tiền xử lý và căn chỉnh nhãn thủ công ---\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "\n",
    "    for tokens, labels in zip(examples[\"tokens\"], examples[\"ner_tags\"]):\n",
    "        # Sử dụng tokenizer để chuyển các token thành input_ids và attention_mask\n",
    "        encoding = tokenizer(tokens, truncation=True, padding=True, max_length=128, is_split_into_words=True)\n",
    "\n",
    "        input_ids = encoding[\"input_ids\"]\n",
    "        attention_mask = encoding[\"attention_mask\"]\n",
    "\n",
    "        # Cập nhật nhãn (label) cho các sub-token\n",
    "        label_ids = []\n",
    "        for token, label in zip(tokens, labels):\n",
    "            word_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(token))\n",
    "            label_ids.append(label_to_id[label])\n",
    "            label_ids.extend([-100] * (len(word_ids) - 1))  # Đánh dấu các sub-token phụ thuộc\n",
    "\n",
    "        # Thêm nhãn cho padding token\n",
    "        label_ids = label_ids[:128]  # Cắt cho phù hợp với max_length (tránh vượt quá giới hạn)\n",
    "\n",
    "        tokenized_inputs[\"input_ids\"].append(input_ids)\n",
    "        tokenized_inputs[\"attention_mask\"].append(attention_mask)\n",
    "        tokenized_inputs[\"labels\"].append(label_ids)\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "print(\"Tokenizing...\")\n",
    "tokenized_datasets = raw_datasets.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# --- DataCollator ---\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# --- DataLoader ---\n",
    "train_loader = DataLoader(tokenized_datasets[\"train\"], batch_size=8, shuffle=True, collate_fn=data_collator)\n",
    "test_loader = DataLoader(tokenized_datasets[\"test\"], batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "# --- Huấn luyện ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "# Hàm đánh giá mô hình\n",
    "def evaluate_model():\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "        for pred, label in zip(predictions, labels):\n",
    "            true_pred = [id_to_label[p.item()] for (p, l) in zip(pred, label) if l != -100]\n",
    "            true_label = [id_to_label[l.item()] for (p, l) in zip(pred, label) if l != -100]\n",
    "            all_preds.append(true_pred)\n",
    "            all_labels.append(true_label)\n",
    "    result = metric.compute(predictions=all_preds, references=all_labels)\n",
    "    print(\"Evaluation:\", result)\n",
    "\n",
    "print(\"Training...\")\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    evaluate_model()\n",
    "\n",
    "print(\"Training done.\")\n",
    "model.save_pretrained(\"./phobert-ner\")\n",
    "tokenizer.save_pretrained(\"./phobert-ner\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
